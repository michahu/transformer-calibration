\documentclass[12pt]{extarticle}
\usepackage[utf8]{inputenc}
\usepackage{cite}

%--------------------Make usable space all of page
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\topmargin}{0in}
\setlength{\headsep}{-.25in}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{8.5in}

%--------------------Indention
\setlength{\parindent}{1cm}
\setlength{\parskip}{0.3cm}

\title{Entropy-Based Calibration of GPT-2}
\author{Michael Hu (myhu), 2021 \\ 
Advisor: Prof. Karthik Narasimhan}
\date{October 2, 2019}

\begin{document}

\maketitle

{\large \textbf{Motivation and Goal}}

Broadly speaking, the motivation is to develop techniques that will help us train better text-generating models. Here, ``better" is generating text that is closer to normal language. (More on this later). The approach of choice here is step-wise calibration. With step-wise calibration, we adjust the model after every generated word, recalibrating it to match some property of the target distribution. Concretely, our goal is to develop an efficient algorithm that calibrates text-generating models. At each step, the algorithm should calibrate the model towards choosing words such that the entropy rate of the model's text is close to that of the underlying language.

{\large \textbf{Problem Background and Related Work}}

A challenge in text generation is creating metrics to evaluate text-generating models. Suppose that a human believes that one model produces better English than the rest. How can we measure this ``goodness" quantitatively? One method is to calculate the entropy rate of the model's text output and compare it against the entropy rate of the underlying language itself. Informally, informational entropy is the average rate at which a data source produces information \cite{shannon-ie}. Knowing this, we can reason that our model should produce information at a rate close to that of the baseline language.

With this in mind, in 2019 Braverman et. al. proposed Algorithm 2, a step-wise ``local entropy rate calibration" algorithm. Essentially, the algorithm calibrates models to choose words that minimize growth in the entropy rate one step in the future. Experimentally, Algorithm 2 decreases entropy growth compared to the baseline model after one iteration \cite{Braverman}. 

{\large \textbf{Approach}}

Algorithm 2 calculates entropy rates for all possible words one step in the future, which is to say that it calculates entropy rates for \textit{all words in the English vocabulary}. This computation is slow. Instead of evaluating all possible outcomes, we propose using sampling approaches or heuristic approaches to choose words instead. 

One such heuristic approach is beam search. Beam search is a best-first search algorithm that keeps track of the $k$ best candidates in every iteration. During each iteration, beam search considers all results from the $k$ candidates. It then chooses the $k$ best of these results for seeds in the next iteration \cite{beam-search}. We could use beam search to evaluate the entropy growth of the $k$ words most probable to come next and pick the word that best minimizes the entropy growth. Concretely, our approach is to use bounds or search algorithms like beam search to decrease the computational complexity of Algorithm 2.

{\large \textbf{Plan}}

We will first attempt to implement Algorithm 2 using beam search instead of an explicit computation over the entire vocabulary. We will then measure the entropy growth of text generated by models trained under this new algorithm, and compare the growth to that of text generated solely by the models themselves. If beam search is unsuccessful, we will consider other techniques, such as other graph search algorithms or simply choosing a fixed number of most probable words to consider. We will then repeat the above process.

{\large \textbf{Evaluation}}

We will evaluate our new algorithm on two metrics: model training time and entropy growth of the generated text. Ideally, we want to develop an algorithm such that model training time is only increased by a small factor, but the entropy rate of the resulting text is closer to that of the underlying language. 

We will be using our algorithm to calibrate GPT-2, a deep-learning-based NLP model that is very good at generating text, qualitatively speaking \cite{radford2019language}. We will measure GPT-2 training times before and after calibration, and compare the entropy rates of the output texts to the underlying language in both cases. Hopefully, we will observe improvement after GPT-2 has been calibrated.

\bibliographystyle{plain}
\bibliography{calibration}

\end{document}
